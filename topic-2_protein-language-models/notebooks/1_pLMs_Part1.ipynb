{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad5bd5f0-34c0-4ffd-9256-c0ffcc2eb8bd",
   "metadata": {},
   "source": [
    "# Protein Language Models Part 1\n",
    "### Learning objectives: \n",
    "- Load a pre-trained pLM\n",
    "- Investigate the internal representation of tokens\n",
    "- Fine-Tune a pLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6223ee4-efa5-4f81-855f-8ed984745fec",
   "metadata": {},
   "source": [
    "## 0. Background\n",
    "### 0.1 Introduction\n",
    "This DeepChem tutorial is designed to serve as an introductory primer on protein language models, a powerful and versatile method of processing protein sequence information inspired by methods from the natural language space. Over the past decade, natural language processing has shown the strength of using learned representations to encapsulate the semantic meaning of text data. Notable models like word2vec [[1]](https://arxiv.org/abs/1301.3781) and GloVe [[2]](https://aclanthology.org/D14-1162/) proved that self-supervised pre-training on large, unlabeled corpora effectively creates robust feature embeddings that maintain similarity and analogy in language. However, these models were limited in utility by their context-free embeddings. The advent of context-aware models, starting with BERT [[3]](https://aclanthology.org/N19-1423/), led to numerous sequence models applicable beyond language domains. In biology, self-supervised pre-training on protein language models has achieved state-of-the-art performance in various tasks by deriving context-aware amino acid embeddings that can be finteuned to capture information on structure [[4]](https://www.biorxiv.org/content/10.1101/2020.12.15.422761v1) and function [[5]](https://www.biorxiv.org/content/early/2023/08/24/2023.08.23.554486.full.pdf) of proteins.\n",
    "\n",
    "This tutorial aims to provide an overview of the concepts and intuition of protein language models that are needed to work with them and understand their input/outputs, strengths, failure modes. We skip over the detailed breakdown of their architecture, but invite the community to add content as they see fit in the form of a pull request to build upon this.\n",
    "\n",
    "**Disclaimer**: For brevity sake, we make some assumptions with familiarity to the multi-layered perceptron, neural networks, and learning by gradient descent. Additionally we assume some fluency with probability theory on matters such as discrete vs. continuous distributions, likelihood, and conditional distributions. We provide links on non-obvious topics and concepts to external sources wherever necessary to bring the audience a vetted and beginner friendly source to start learning on the more complicated topics. Follow along for a high-level overview into the reason that protein language models have been so successful across a broad range of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e4ae74-e8fe-4f61-a470-5a8ff246529f",
   "metadata": {},
   "source": [
    "### 0.2 What is a language model?\n",
    "Under the hood, all language models are nothing more than probability distributions over tokens, or discrete sub-sequences. In natural language, a very intuitive set of tokens are the words of a language, or perhaps even the characters. Both have their own pros and cons. For simplicity, let's work with words as tokens here, though this changes for proteins. Since the learned distribution is over discrete units (words), this distribution is a [categorical distribution](https://en.wikipedia.org/wiki/Categorical_distribution), not a [continuous](https://en.wikipedia.org/wiki/Probability_distribution) one. To make this more concrete, take for example a common language model that you have likely interacted with many times in your life: text auto-complete. Text auto-complete is a conditional language model that takes the previous words you have written and then computes the [conditional probability](http://www.stat.yale.edu/Courses/1997-98/101/condprob.htm) over all the words in its vocabulary and returns the highest probability words based on the context. If you'd like a very intuitive and fine-grained explanation for both the form and function of language models, the 3Blue1Brown [walk-through](https://www.3blue1brown.com/lessons/gpt) is a great resource that breaks down the basics of the architecture, the flow of information, and the process of training a specific LLM (GPT). In this section we skip over the architecture of the language models and instead leave them as a black-box, focusing more on the how language models learn from sequences to better motivate their use in the protein domain.\n",
    "\n",
    "A simple way to visualize what a language model is doing in the background is to think of the language model as updating and indexing a huge square matrix of [transition probabilities](https://en.wikipedia.org/wiki/Stochastic_matrix) of size $D x D$, where $D$ is the vocabulary size of the model. Here vocabulary size refers to the number of unique words or sub-words that make up the state space of the categorical distribution. So a model that only knows the words ['a', 'boy', 'cute', 'is', 'student', 'the' 'walking'] has a vocabulary size of 7. If we start off with an untrained model that is randomly initialized, we can use a [uniform](https://www.investopedia.com/terms/u/uniform-distribution.asp) initialization we would get a transition matrix that looks something like this where we introduce a special word to designate the end of sequence (EOS):\n",
    "\n",
    "|         | a    | boy  | cute | is   |student| the     | walking | EOS |\n",
    "|---------|------|------|------|------|-------|---------|---------|-----|\n",
    "| a       | 0.125| 0.125| 0.125| 0.125| 0.125 | 0.125   | 0.125   |0.125|\n",
    "| boy     | 0.125| 0.125| 0.125| 0.125| 0.125 | 0.125   | 0.125   |0.125|\n",
    "| cute    | 0.125| 0.125| 0.125| 0.125| 0.125 | 0.125   | 0.125   |0.125|\n",
    "| is      | 0.125| 0.125| 0.125| 0.125| 0.125 | 0.125   | 0.125   |0.125|\n",
    "| student | 0.125| 0.125| 0.125| 0.125| 0.125 | 0.125   | 0.125   |0.125|\n",
    "| the     | 0.125| 0.125| 0.125| 0.125| 0.125 | 0.125   | 0.125   |0.125|\n",
    "| walking | 0.125| 0.125| 0.125| 0.125| 0.125 | 0.125   | 0.125   |0.125|\n",
    "\n",
    "\n",
    "However, if we look at some of the transition probabilities, we can immediately see that the model is not very good. For example, the probability of the word 'a' coming after 'a' should be close to 0. Same goes for the word 'the' coming after 'a'. It's pretty clear that we need some way of training this model so that we can get some realistic transition probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e8566d-30ec-4ec0-9d06-1de487ae2170",
   "metadata": {},
   "source": [
    "### 0.3 Methods for learning language\n",
    "#### 0.3.1 Causal learning\n",
    "The first language models were trained on the principle of [causal language modeling](https://huggingface.co/docs/transformers/en/tasks/language_modeling), where the model is tasked with next word prediction during each training step.\n",
    "\n",
    "$$\\text{The quick brown fox jumped over the lazy -----.}$$\n",
    "$$ P(x_t|x_{<t}) = ? $$\n",
    "\n",
    "After enough rounds of this training protocol the model learns a much more plausible distribution over the words - something that looks like the following:\n",
    "\n",
    "|         | a    | boy  | cute | is   |student |  the  | walking| EOS  |\n",
    "|---------|------|------|------|------|---------|-------|--------|-----|\n",
    "| a       | 0.0  | 0.5  | 0.1  | 0.05 | 0.25    | 0.05  |0.05    | 0.0 |\n",
    "| boy     | 0.15 | 0.0  | 0.1  | 0.4  | 0.05    | 0.15  |.05     | 0.1 |\n",
    "| cute    | 0.05 | 0.2  | 0.0  | 0.1  | 0.25    | 0.1   |0.0     | 0.3 |\n",
    "| is      | 0.2  | 0.0  | 0.3  | 0.0  | 0.0     | 0.2   |0.3     | 0.0 |\n",
    "| student | 0.15 | 0.05 | 0.1  | 0.5  | 0.0     | 0.05  |.05     | 0.1 |\n",
    "| the     | 0.0  | 0.5  | .2   | 0.0  | 0.25    | 0.0   |0.05    | 0.0 |\n",
    "| walking | 0.1   | .0   | .05  | .2   | .05    | 0.35 |0.0      | 0.3 |\n",
    "| EOS     | 0.0  | 0.0   | 0.0 | 0.0   | 0.0    | 0.0   | 0.0    | 1.0 |\n",
    "\n",
    "Here we can see that the model has learned that the words above are not typically repeated twice in a row. It assigns subject words ['boy', 'student'] after the word 'the' with higher probability than the verbs ['is', 'walking']. If we start at 'the' and sample the most likely words at each transition we can generate the following sentence as a path through the model: 'the' -> 'boy' -> 'is' -> 'walking' -> 'EOS'. This mode of sampling a word at every time step and then conditioning on the previously sampled words is known as auto-regressive generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395e53fe-d3d6-462f-bbf7-a65a4089a639",
   "metadata": {},
   "source": [
    "### 0.3.2 The power of neural networks\n",
    "A key point here that motivates the use of increasingly complicated neural networks for language modeling tasks is that with our illustrative example, we have the transition probability matrix and each step we sample from it like a [markov chain](https://en.wikipedia.org/wiki/Markov_chain). However, there are longer range dependencies in language that are not captured simply by conditioning on the previous word. So why not construct matrices that map transition probabilities between pairs of words or triples, or even more? Beyond issues of computational feasibility, this model would require that all possible n-grams would have been seen at least once during training, which greatly limits the models learning and usability. Neural networks have emerged as a great way of using large contexts and generating a neural representation of the sequence before indexing a loose transition matrix that maps between the neural representation and all the words in the vocabulary. For this reason, we often see in the language model space the distribution over the vocabulary from the last layer of a neural network as a one-dimensional probability distribution rather than a probability matrix. Keeping this in mind, we can see how these language models accommodate another method of learning language that draws from the context before **AND** after a word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f5f79a-cfbe-4868-8c7e-3bfe2faeed12",
   "metadata": {},
   "source": [
    "### 0.3.3 Masked language modelling\n",
    "Causal language modeling has a key drawback in that sometimes the necessary context to make sense of a word in a sentence comes after the word and not before. Masked language modeling is like causal modeling, but makes use of the fact that context may come before and after the word of interest.\n",
    "\n",
    "$$\\text{The quick brown [MASK] jumped over the lazy dog.}$$\n",
    "$$ P(x_t|x_{! t}) = ??$$\n",
    "\n",
    "This approach is what underlies the powerful BERT [[3]](https://arxiv.org/pdf/1810.04805) language model, where they used a masking rate of about 15\\% of the words. Amazingly, this approach has been tried on sequences other than language and has been shown to be a robust model for learning the syntax and semantics of sequential data of various modalities including time series data, videos, and yes even proteins!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d511491-bc72-4d94-8ad7-54e47daa8b98",
   "metadata": {},
   "source": [
    "### 0.4 How do protein language models (pLMs) work?\n",
    "Inspired by the success of [large language models (LLMs)](https://magazine.sebastianraschka.com/p/understanding-large-language-models) in a broad variety of natural language tasks, protein language models represent a powerful new approach in understanding the syntax and semantics of protein sequences [[6]](https://arxiv.org/abs/2007.06225). These models are trained on using the masked language modeling objective to mask out portions of the sequence and infer what amino acids belong across billions of protein sequences, learning to identify patterns and relationships within the sequences that are crucial for their structure and function. This step is called pre-training and it imbues the language model with a general understanding of the structural dependencies within the language, in this case proteins.\n",
    "\n",
    "An optional second training step known as [fine-tuning](https://medium.com/@bijit211987/the-evolution-of-language-models-pre-training-fine-tuning-and-in-context-learning-b63d4c161e49) can be applied on a pre-trained protein language model, to further train it on a specific task with protein sequence examples annotated with labels. In practice, starting from the pretrained weights has shown to have better performance than starting from randomly initialized weights as the model simply learns how to use strong representations of the inputs (learned during pretraining) instead of jointly learning the representation AND how to use it.  PLMs finetuned on the mappings between specific protein families or functional classes can significantly enhance predictive power compared to non-pretrained models, and can be applied in a number of different use cases, such as predicting binding sites or the effects of mutations.\n",
    "\n",
    "\n",
    "One of the most compelling benefits of PLMs is their ability to capture coevolutionary relationships within and across protein sequences [[7]](https://www.biorxiv.org/content/10.1101/2024.01.30.577970v1.full.pdf). In the same way that words in a sentence co-occur to convey coherent meaning, amino acid residues in a protein sequence co-evolve to maintain the protein's structural integrity and functionality. PLMs capture these coevolutionary patterns, allowing for the prediction of how changes in one part of a protein may affect other parts. Thus, from a design perspective, the directed evolution task is an area where PLMs offer substantial advantages. In a directed evolution experiment, a naturally occurring protein can be mutated according to any arbitrary heuristic and is then checked if a desired function has improved. Since PLMs capture intra-sequence conditional distributions, this process can be vastly streamlined by masking portions of the protein we wish to 'mutate' and sampling from the distribution of what amino acids are strong candidates to occur given the rest of the sequence. PLMs thus have the potential to significantly reduce experimental burden by identify promising candidates a higher hit rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd082d3a-1721-40f2-8a32-3940c9432a70",
   "metadata": {},
   "source": [
    "#### 0.4.1 Reconciling Sequence and Structure\n",
    "\n",
    "Some protein language models combine in their training input amino acid sequence data with structural data, such as 3D coordinates of atoms in the protein. The goal is to explicitly incorporate structural information information, aiming to enhance the representation and ultimately prediction of unseen protein structure and functions. This is in contrast to sequence-only models that implicitly model structure which is more closely conserved across proteins via homology.\n",
    "\n",
    "Models like ESM-1b [[4]](https://www.biorxiv.org/content/10.1101/2020.12.15.422761v1.full.pdf) and ESM-2 [[8]](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v3) are examples of sequence-only pLMs that do not explicitly incorporate 3D structural information. These sequence-based pLMs have demonstrated impressive performance on a variety of protein function prediction tasks by learning patterns from large protein sequence datasets.\n",
    "However, the lack of structural information can limit the generalization capabilities of sequence-only PLMs. This is true especially for applications heavily dependent on protein structure such as contact prediction. Moreoever, the inclusion of structural information helps overcome the distributional biases that exist in the training datasets of sequences.\n",
    "\n",
    "Structure-aware pLMs like S-PLM[[9]](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10441326/) and ESM-Fold [[8]](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v3) are trained on both sequence and structural information, and in turn generate protein representations that encode both sequence and structural information. These models use various methods such as multi-view contrastive learning to align the sequence and structure representations in a shared latent space (S-PLM). The structural awareness enables them to achieve comparable or superior performance to specialized structure-based methods or sequence-based pLMs, particularly for applications that heavily rely on protein structure.\n",
    "\n",
    "Interestingly, the recently released ESM-3 [[10]](https://www.evolutionaryscale.ai/blog/esm3-release) pLM reasons over sequence, structure, and function, meaning that for each protein, its sequence, structure, and function are extracted, tokenized, and partially masked during pre-training.\n",
    "\n",
    "![image.png](https://www.biorxiv.org/content/biorxiv/early/2024/05/13/2023.08.06.552203/F1.large.jpg?width=800&height=600&carousel=1)\n",
    "*The framework of S-PLM and lightweight tuning strategies for downstream supervised learning. a, The framework of S-PLM: During pretraining, the model inputs both the amino acid sequences and contact maps derived from protein structures simultaneously. After pretraining, the ESM-Adapter that generates the AA-level embeddings before the projector layer is used for downstream tasks. The entire ESM-Adapter model can be fully frozen or learnable through lightweight tuning. b, Architecture of the ESM-Adapter. c, Adapter tunning for supervised downstream tasks. d, LoRA tuning for supervised downstream tasks is implemented. Adapted from [[9]](https://www.biorxiv.org/content/10.1101/2023.08.06.552203v3).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6490a46-1e86-4582-b675-c4f5437e4d30",
   "metadata": {},
   "source": [
    "## 0.5 MSA-aware vs non-MSA-aware protein language models\n",
    "Multiple Sequence Alignment (MSA) is a method used to align three or more biological sequences (protein or nucleic acid) to identify regions of similarity that may indicate functional, structural, or evolutionary relationships. MSAs are a cornerstone in bioinformatics for tasks such as phylogenetic analysis, structure prediction, and function annotation.\n",
    "\n",
    "In the context of pLMs, MSA provides evolutionary context to the representations of protein sequences. PLMs can be MSA-aware and non-MSA-aware:\n",
    "#### MSA-aware models\n",
    "\n",
    "MSA-aware models, such as the MSA Transformer [[11]](https://proceedings.mlr.press/v139/rao21a.html), Evoformer (used in AlphaFold) [[12]](https://www.nature.com/articles/s41586-021-03819-2) and ESM-MSA [[11]](https://proceedings.mlr.press/v139/rao21a.html), are trained on datasets that include MSAs as input to incorporate evolutionary information and relationships between sequences to learn richer representations. They align multiple homologous sequences to capture conserved and variable regions. The rationale is that conserved regions often indicate functionally or structurally important parts of the protein, while variable regions can provide insights into evolutionary divergence and adaptation.\n",
    "\n",
    "MSA-aware models can provide deeper insights into protein function and structure due to the evolutionary context. However, they are computationally intensive and require high-quality MSAs, which may not be available for all protein families."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9842bca1-79ae-4695-9fdc-68308c12b8a2",
   "metadata": {},
   "source": [
    "#### Non-MSA-aware models\n",
    "Non-MSA-aware models, such as ESMFold (ESM-2)[[8]](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v3), ProtBERT [[6]](https://arxiv.org/abs/2007.06225) and TAPE, treat each protein sequence independently and do not explicitly incorporate evolutionary information from MSAs. They are trained on large datasets of individual protein sequences, learning patterns and representations directly from the sequence data.\n",
    "\n",
    "While they can generalize well to diverse sequences and are computationally efficient, they may miss out on the evolutionary context that can be crucial for certain tasks.\n",
    "\n",
    "<img src=\"https://cdn.prod.website-files.com/621e95f9ac30687a56e4297e/64a8d21628b03e0f9f71a4fc_V2_1677884143661_5f927638-5fb9-40ac-9301-8f25c3bcf649.png\" alt=\"image.png\" width=\"800\">\n",
    "\n",
    "\"*Multiple Sequence Alignment\". BioRender.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f9e01d-ec8a-4741-8dbf-db0ab29e05f7",
   "metadata": {},
   "source": [
    "#### Benefits and challenges of MSA-aware models:\n",
    "- Evolutionary insight: MSAs provide evolutionary information, highlighting conserved residues that are often critical for protein function and structure.\n",
    "- Improved predictions: By incorporating evolutionary context, MSA-aware models can improve performance on tasks such as secondary structure prediction, contact prediction, and function annotation.\n",
    "- Functional and structural understanding: MSAs help in identifying functionally important regions and understanding the structural constraints of proteins.\n",
    "- Computational complexity: Generating and processing MSAs is computationally expensive and time-consuming.\n",
    "- Data availability: High-quality MSAs are not available for all protein families, especially those with few known homologs.\n",
    "- Model complexity: MSA-aware models are more complex and require sophisticated architectures to effectively utilize the evolutionary information.\n",
    "\n",
    "Other considerations:\n",
    "- The performance benchmark of both MSA-aware and not MSA-aware for predicting the 3D structure of proteins, as well as their function and other properties is currently an active topic of research.\n",
    "- Interestingly, MSA-free models have reported ability to efficiently generate sufficiently accurate MSAs that can be used as input for the MSA-aware models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77a276b-1a57-42a5-a491-c25575b8f825",
   "metadata": {},
   "source": [
    "Let's see how this works hands-on!\n",
    "------------------------------------------------\n",
    "## 1. Investigate protein representation in the pLM ProtBERT\n",
    "(Adapted from [DeepChem Tutorials](https://github.com/deepchem/deepchem/blob/master/examples/tutorials/ProteinLM_Tutorial0.ipynb), check the original notebook for more information.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef473db-ba2d-4c8c-8a18-723d87171dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "from transformers import BertForMaskedLM, BertTokenizer, pipeline\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35948bba-b323-470d-a416-2201c3b2fb6d",
   "metadata": {},
   "source": [
    "### Proteins of interest to be investigated\n",
    "![hemoglobin.png](https://www.researchgate.net/profile/Lakna-Panawala/publication/313841668/figure/fig1/AS:463461898559488@1487509335507/Structure-of-Hemoglobin.png)\n",
    "\n",
    "Image Source: *Adapted from \"Représentation simplifiée de l'hémoglobine et de l'hème\". Wikimedia Commons.*\n",
    "\n",
    "\n",
    "Hemoglobin is the protein responsible for transporting oxygen from the lungs to all the cells of our body via red blood cells. Hemoglobin is a great protein to interrogate the behaviors of protein language models as it is highly conserved in certain regions across species, and also slightly variable in other places. What would we expect the distribution over amino acids to look like if we mask out a highly conserved region? What about a highly diverse region? Let's find out.\n",
    "\n",
    "\n",
    "**Hemoglobin Sequence Homology across closely related mammals** (from [[13]](https://www.nature.com/articles/s41598-019-50619-w)):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75253b20-067c-4fc5-9092-feddaff1b6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hemoglobin_beta = {\n",
    "'human':\n",
    "\"MVHLTPEEKSAVTALWGKVNVDEVGGEALGRLLVVYPWTQRFFESFGDLSTPDAVMGNPKVKAHGKKVLGAFSDGLAHLDNLKGTFATLSELHCDKLHVDPENFRLLGNVLVCVLAHHFGKEFTPPVQAAYQKVVAGVANALAHKYH\",\n",
    "'chimpanzee':\n",
    "\"MVHLTPEEKSAVTALWGKVNVDEVGGEALGRLLVVYPWTORFFESFGDLSTPDAVMGNPKVKAHGKKVLGAFSDGLAHLDNLKGTFATLSELHCDKLHVDPENFRLLGNVLVCVLAHHFGKEFTPPVQAAYQKVVAGVANALAHKYH\",\n",
    "'camel':\n",
    "\"MVHLSGDEKNAVHGLWSKVKVDEVGGEALGRLLVVYPWTRRFFESFGDLSTADAVMNNPKVKAHGSKVLNSFGDGLNHLDNLKGTYAKLSELHCDKLHVDPENFRLLGNVLVVVLARHFGKEFTPDKQAAYQKVVAGVANALAHRYH\",\n",
    "'rabbit':\n",
    "\"MVHLSSEEKSAVTALWGKVNVEEVGGEALGRLLVVYPWTQRFFESFGDLSSANAVMNNPKVKAHGKKVLAAFSEGLSHLDNLKGTFAKLSELHCDKLHVDPENFRLLGNVLVIVLSHHFGKEFTPQVQAAYQKVVAGVANALAHKYH\",\n",
    "'pig':\n",
    "\"MVHLSAEEKEAVLGLWGKVNVDEVGGEALGRLLVVYPWTQRFFESFGDLSNADAVMGNPKVKAHGKKVLQSFSDGLKHLDNLKGTFAKLSELHCDQLHVDPENFRLLGNVIVVVLARRLGHDFNPNVQAAFQKVVAGVANALAHKYH\",\n",
    "'horse':\n",
    "\"*VQLSGEEKAAVLALWDKVNEEEVGGEALGRLLVVYPWTQRFFDSFGDLSNPGAVMGNPKVKAHGKKVLHSFGEGVHHLDNLKGTFAALSELHCDKLHVDPENFRLLGNVLVVVLARHFGKDFTPELQASYQKVVAGVANALAHKYH\",\n",
    "'bovine':\n",
    "\"M**LTAEEKAAVTAFWGKVKVDEVGGEALGRLLVVYPWTQRFFESFGDLSTADAVMNNPKVKAHGKKVLDSFSNGMKHLDDLKGTFAALSELHCDKLHVDPENFKLLGNVLVVVLARNFGKEFTPVLQADFQKVVAGVANALAHRYH\",\n",
    "'sheep':\n",
    "\"M**LTAEEKAAVTGFWGKVKVDEVGAEALGRLLVVYPWTQRFFEHFGDLSNADAVMNNPKVKAHGKKVLDSFSNGMKHLDDLKGTFAQLSELHCDKLHVDPENFRLLGNVLVVVLARHHGNEFTPVLQADFQKVVAGVANALAHKYH\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea0d88e-efd3-4d90-b966-3d6dc8c761fe",
   "metadata": {},
   "source": [
    "As we can see there is a great degree of overlap between the hemoglobin $\\beta$ subunits across the animal kingdom. The part of the hemoglobin sequence that is essential to the function of carrying oxygen is the part that binds to the heme group. This is handled by a single amino acid, namely the Histidine (H) near position 92 on the beta chain, in the middle of the underlined subsequences above. Unsurprsingly, given its functional importance, the amino acid (H) at position is unchanged across all species. Can a language model recapitulate this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c16b7d-12a4-4613-af66-e5052da72e44",
   "metadata": {},
   "source": [
    "### Load the model\n",
    "[ProtBERT](https://arxiv.org/abs/2007.06225) is a protein language model based on the BERT model.\n",
    "Load ProtBERT, use the [pre-trained Uniref100 Model](https://huggingface.co/Rostlab/prot_bert). Also load the tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef0b9f-4821-457c-a5e1-12e5281cd21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "model = BertForMaskedLM.from_pretrained(\"Rostlab/prot_bert\", weights_only=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1af51-2619-46bd-ab65-88bfb6609693",
   "metadata": {},
   "source": [
    "### **ProtBERT learned representation of Hemoglobin 𝛽**\n",
    "\n",
    "ProtBERT [[6]](https://arxiv.org/abs/2007.06225) is a BERT style protein language model that was trained via masked amino acid modeling on Uniref100 [[14]](https://doi.org/10.1093/bioinformatics/btm098), a dataset consisting of 217 million protein sequences, and 88B amino acids. The Uniref database contains deduplicated protein sequences from UniProt where they are clustered together, and thus deduplicated, given the threhold of sequence identity between species. Uniref100 takes 100% sequence identity, while Uniref90 does 90% and Uniref50% has a cutoff of 50%. As such, ProtBERT was trained on the largest of these databases. Lets load up ProtBERT and see what it looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cfe126-740e-4c88-94af-6319cfa554a8",
   "metadata": {},
   "source": [
    "### See how the model recovers masked positions in Hemoglobin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8269966-c072-4821-9508-7c8be2c00851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask the F8 Histidine of Hemoglobin B Subunit\n",
    "human_heme = list(hemoglobin_beta['human'])\n",
    "human_heme[92] = \"[MASK]\"\n",
    "masked_heme = ' '.join(human_heme)\n",
    "print(masked_heme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32758e6e-488a-42e3-ae40-473655256166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise the sequence and pass it through the model\n",
    "tokenized_sequence = tokenizer(masked_heme, return_tensors='pt')\n",
    "tokenized_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7375b3-e23b-488c-8367-5ddcb9461dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise the sequence and pass it through the model\n",
    "model_outs = model(**tokenized_sequence)\n",
    "model_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0433cee-b7f0-4c24-9f70-7167ef34e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the logits\n",
    "logits = model_outs.logits.squeeze()[1:-1] # Ignore SOS and EOS special tokens\n",
    "print(logits.shape)\n",
    "softmaxed = F.softmax(logits, dim=1).detach().numpy() # Softmax to normalize the logits to sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f1d70-9faf-4439-986b-62967b261065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode the Logits Using Greedy Decoding (Max Probability at Each Timestep)\n",
    "decoded_outputs = tokenizer.batch_decode(softmaxed.argmax(axis=1))\n",
    "decoded_sequence = ''.join(decoded_outputs)\n",
    "print(decoded_sequence)\n",
    "print(f'The filled-in masked sequence is: {decoded_sequence[92]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e87986a-87d7-46c8-901a-f9fa1315321b",
   "metadata": {},
   "source": [
    "**Sanity Check:** Looks like the pLM ProtBERT was able to recapitulate the correct amino acid at that position. But how confident was the model? Let's visualize the distribution at that position and see what other amino acids the  model was choosing between.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5e3772-761f-4a25-b543-c8f9f1171654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the Token Distribution at the F8 Histidine\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(tokenizer.get_vocab().keys(), softmaxed[92])\n",
    "plt.ylabel('Normalized Probability')\n",
    "plt.xlabel('Model Vocabulary')\n",
    "plt.title('Target Distribution at the F8 Histidine')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca36c48f-f5b4-4b40-b7df-a8460654f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the Logits Map Across All Positions\n",
    "plt.figure(figsize=(10,16))\n",
    "sns.heatmap(softmaxed, xticklabels=tokenizer.get_vocab())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d32dfb3-f1b9-40f3-a71b-c1176c698297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at a Low Confidence Region\n",
    "\n",
    "plt.bar(tokenizer.get_vocab().keys(), softmaxed[87])\n",
    "plt.ylabel('Normalized Probability')\n",
    "plt.xlabel('Model Vocabulary')\n",
    "plt.title('Target Distribution at Position 87')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d2169d-0115-4e52-bb82-47a6bc86ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for animal in hemoglobin_beta:\n",
    "    print(f'{animal} has residue {hemoglobin_beta[animal][87]} at position 87')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f8377-0eef-40d8-84e8-3f132e1b30bc",
   "metadata": {},
   "source": [
    "------------------------------------------------\n",
    "## Optional: To run the second part we need Python <3.12 and the newest DeepChem version. Skip if setting up the env is too tidious.\n",
    "## 2. Fine-Tune ProtBERT for water solubility\n",
    "(Adapted from [DeepChem Tutorials](https://github.com/deepchem/deepchem/blob/master/examples/tutorials/Introduction_to_ProtBERT.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a9114f-b37f-4b02-a3e9-6b03807294ef",
   "metadata": {},
   "source": [
    "Use the [DeepLoc](https://academic.oup.com/bioinformatics/article/33/21/3387/3931857?login=false) dataset for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d6332-dca3-4d99-9c3c-f84b01eba5bf",
   "metadata": {},
   "source": [
    "### Understanding ProtBERT\n",
    "\n",
    "ProtBERT is a specialized variant of the BERT (Bidirectional Encoder Representations from Transformers) model, specifically designed for processing protein sequences. Developed by researchers at the Rostlab, ProtBERT leverages the transformative capabilities of BERT to encode the complex and nuanced features present in amino acid sequences.\n",
    "\n",
    "#### Key Features of ProtBERT:\n",
    "\n",
    "1. **BERT Architecture Adaptation:** ProtBERT adapts the original BERT architecture to the unique characteristics of protein sequences. It consists of transformer layers that capture both local and global dependencies in the sequence, making it suitable for tasks ranging from masked language modeling (MLM) to sequence classification.\n",
    "\n",
    "2. **Tokenization and Embedding:** Similar to how BERT tokenizes words in natural language, ProtBERT tokenizes amino acids in protein sequences. It uses a specialized tokenizer trained on large protein sequence databases, enabling it to generate embeddings that capture the semantic meaning and context of amino acids.\n",
    "\n",
    "3. **Pretraining and Fine-tuning:** ProtBERT supports pretraining on large-scale protein datasets such as UniRef100 and BFD (Baker's finite difference), which helps it learn representations that generalize well across diverse protein sequences. These pretrained models can then be fine-tuned for specific tasks like protein classification (e.g., predicting membrane proteins or subcellular localization). The authors first pretrain on protein sequences with lengths less than 512, then on sequences less than 1024, and finally on sequences up to 40,000.\n",
    "4. **Task-specific Adaptation:** Depending on the task, ProtBERT can be adapted with different classifier heads. For instance, it can be configured for single-label or multi-label classification tasks, allowing researchers to tailor it to specific biological questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3992a5-a00f-4f8a-acbb-7ad7dce7f529",
   "metadata": {},
   "source": [
    "### Loading ProtBERT\n",
    "\n",
    "ProtBERT comes pretrained with models specifically trained on the Uniref100 and BFD datasets. These pretrained models are available for both Masked Language Modeling (MLM) and Sequence Classification tasks. This section covers how to load ProtBERT in different modes and provides details about the pretrained datasets available.\n",
    "\n",
    "#### Pretrained Models:\n",
    "\n",
    "1. **Uniref100 Model:**\n",
    "   - **Description:** The Uniref100 model is pretrained on the Uniref100 [1] dataset.\n",
    "   - **Usage:** Initialize ProtBERT with `model_path = 'Rostlab/prot_bert'` to load the Uniref100 pretrained model.\n",
    "\n",
    "2. **BFD Model:**\n",
    "   - **Description:** The BFD model is pretrained on the BFD(Big Fantastic Database) dataset [2][3].\n",
    "   - **Usage:** Initialize ProtBERT with `model_path = 'Rostlab/prot_bert_bfd'` to load the BFD pretrained model.\n",
    "\n",
    "#### Supported Modes:\n",
    "\n",
    "1. **Masked Language Modeling (MLM):**\n",
    "   - **Description:** ProtBERT learns to predict masked amino acids in protein sequences, facilitating a deeper understanding of amino acid relationships and sequence contexts.\n",
    "   - **Usage:** Initialize ProtBERT with `task='mlm'` and specify either `model_path = 'Rostlab/prot_bert'` or `model_path = 'Rostlab/prot_bert_bfd'` for MLM tasks.\n",
    "\n",
    "2. **Sequence Classification:**\n",
    "   - **Description:** Enables classification tasks such as predicting membrane proteins, subcellular localization, or custom classifications using a user-defined classifier head.\n",
    "   - **Usage:** Set `task='classification'` to utilize ProtBERT for sequence classification. Specify the `cls_name` parameter as 'LogReg', 'FFN', or 'custom' to use Logistic Regression, a simple 1-layer FFN, or a custom classifier network, respectively.\n",
    "     - **Custom Task:** Set `cls_name='custom'` and provide a custom classifier head using the `classifier_net` argument. This allows users to apply a custom classifier head on top of the pretrained ProtBERT model.\n",
    "\n",
    "References:\n",
    "\n",
    "[1] Suzek, Baris E., et al. \"UniRef: comprehensive and non-redundant UniProt reference clusters.\" Bioinformatics 23.10 (2007): 1282-1288.\n",
    "\n",
    "[2] Steinegger, Martin, Milot Mirdita, and Johannes Söding. \"Protein-level assembly increases protein sequence recovery from metagenomic samples manyfold.\" Nature methods 16.7 (2019): 603-606.\n",
    "\n",
    "[3] Steinegger, Martin, and Johannes Söding. \"Clustering huge protein sequence sets in linear time.\" Nature communications 9.1 (2018): 2542."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b930357-8e3e-4ffe-a325-29f5de032070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import deepchem as dc\n",
    "import pandas as pd\n",
    "from deepchem.models.torch_models import ProtBERT\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from urllib.request import urlopen\n",
    "from rich.progress import Progress, TransferSpeedColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e191a-b9e7-4ced-8ba3-d06070574c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## datasets for fine-tuning\n",
    "URL_test = \"https://deepchemdata.s3.us-west-1.amazonaws.com/datasets/DeepLoc_test.csv\"\n",
    "URL_train = \"https://deepchemdata.s3.us-west-1.amazonaws.com/datasets/DeepLoc_train.csv\"\n",
    "out_test = './datasets/DeepLoc_test.csv'\n",
    "out_train = \"./datasets/DeepLoc_train.csv\" \n",
    "os.makedirs('datasets', exist_ok=True)\n",
    "\n",
    "print(URL_test)\n",
    "with Progress(*Progress.get_default_columns(), TransferSpeedColumn()) as progress:\n",
    "    with urlopen(URL_test) as res:\n",
    "        length = int(res.headers[\"Content-Length\"])\n",
    "        with progress.wrap_file(res, total=length) as src, open(out_test, \"wb\") as dest:\n",
    "            shutil.copyfileobj(src, dest)\n",
    "print(URL_train)\n",
    "with Progress(*Progress.get_default_columns(), TransferSpeedColumn()) as progress:\n",
    "    with urlopen(URL_train) as res:\n",
    "        length = int(res.headers[\"Content-Length\"])\n",
    "        with progress.wrap_file(res, total=length) as src, open(out_train, \"wb\") as dest:\n",
    "            shutil.copyfileobj(src, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5f9ab5-da0a-4606-94c0-44498902fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demo purpose we choose a subset of the orginal data\n",
    "train_df = pd.read_csv(out_train)\n",
    "string_lengths = train_df[\"protein\"].apply(len)\n",
    "filtered_train_df = train_df[string_lengths < 200].sample(5000)\n",
    "filtered_train_df.to_csv(\"./datasets/DeepLoc_train_5000.csv\",index=False)\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(out_test)\n",
    "string_lengths = test_df[\"protein\"].apply(len)\n",
    "filtered_test_df = test_df[string_lengths < 200].sample(1000)\n",
    "filtered_test_df.to_csv(\"./datasets/DeepLoc_test_1000.csv\",index=False)\n",
    "\n",
    "filtered_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40827a3c-e1d8-4fd7-b22e-06ca0a7e72e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# featurise the \n",
    "featurizer = dc.feat.DummyFeaturizer()\n",
    "tasks = [\"water soluble\"]\n",
    "loader = dc.data.CSVLoader(tasks=tasks,\n",
    "                            feature_field=\"protein\",\n",
    "                            featurizer=featurizer)\n",
    "deeploc_train_dataset = loader.create_dataset(\"./datasets/DeepLoc_train_5000.csv\")\n",
    "deeploc_test_dataset  = loader.create_dataset(\"./datasets/DeepLoc_test_1000.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6148bbbc-b204-4292-ab66-66154827eeaf",
   "metadata": {},
   "source": [
    "### Load the model\n",
    "Load [ProtBERT](https://academic.oup.com/bioinformatics/article/38/8/2102/6502274?login=false), use the [pre-trained Uniref100 Model](https://huggingface.co/Rostlab/prot_bert). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f548389-8396-46b5-bc2c-457cca9ceb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir for finetuning\n",
    "finetune_model_dir = \"finetuning/\"\n",
    "\n",
    "# Network for custom classfication task\n",
    "custom_network = nn.Sequential(nn.Linear(1024, 512),\n",
    "                               nn.ReLU(), nn.Linear(512, 256),\n",
    "                               nn.ReLU(), nn.Linear(256, 2)) \n",
    "\n",
    "# ProtBERT model that can be used for fine-tuning for a downstream task\n",
    "ProtBERTmodel_for_classification = ProtBERT(task='classification',\n",
    "                                            model_path=\"Rostlab/prot_bert\",\n",
    "                                            n_tasks=1,\n",
    "                                            cls_name=\"custom\",\n",
    "                                            classifier_net=custom_network,\n",
    "                                            n_classes=2,\n",
    "                                            model_dir=finetune_model_dir,\n",
    "                                            batch_size=32,\n",
    "                                            learning_rate=1e-5,\n",
    "                                            log_frequency = 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e48bef-1592-43ca-a6c5-04cad02c4029",
   "metadata": {},
   "source": [
    "### Fine-tune the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b84004-9de4-44b8-b92d-6a6bdfbbb7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze underlying ProtBERT and only train the classfier head\n",
    "for param in ProtBERTmodel_for_classification.model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# track the loss\n",
    "all_losses = []\n",
    "loss = ProtBERTmodel_for_classification.fit(deeploc_train_dataset, nb_epoch=1,all_losses = all_losses)\n",
    "\n",
    "# Plot training loss\n",
    "batches = list(range(5, 5 * (len(all_losses) + 1), 5))\n",
    "plt.plot(batches, all_losses, linestyle='-', color='b')\n",
    "plt.title('Training Loss over Batches')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d451a7-6bb9-4635-9ce0-743163a63457",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "Use the deepchem metrics (e.g. accuracy score) to evaluate your final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a79caee-6f0f-48b4-88c8-246d3ef56c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_metric = dc.metrics.Metric(dc.metrics.accuracy_score)\n",
    "eval_score = ProtBERTmodel_for_classification.evaluate(deeploc_test_dataset, [classification_metric],n_classes=2)\n",
    "eval_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ec187e-8102-48b4-a549-6533fbe34342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
