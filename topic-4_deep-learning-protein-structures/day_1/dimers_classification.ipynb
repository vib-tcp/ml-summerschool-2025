{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJtKd8pZX7bZ"
      },
      "source": [
        "# Advanced Protein Dimer Classification with PyTorch\n",
        "\n",
        "This notebook demonstrates advanced neural network techniques to improve performance on the dimers_features.csv dataset, including normalization, dropout, and model size comparisons."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch pandas numpy seaborn scikit-learn"
      ],
      "metadata": {
        "id": "xUdce2alX_EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBNSG5AuX7ba"
      },
      "source": [
        "## Learning Objectives\n",
        "- Implement advanced normalization techniques (BatchNorm, LayerNorm, GroupNorm)\n",
        "- Use different dropout strategies and regularization methods\n",
        "- Compare model performance based on architecture size\n",
        "- Apply advanced training techniques (learning rate scheduling, early stopping)\n",
        "- Analyze model complexity vs. performance trade-offs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eI596ANgX7bb"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.metrics import precision_recall_curve, roc_curve\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "SEED = 42\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Check PyTorch version and CUDA availability\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "def set_plot_params():\n",
        "    plt.style.use(\"seaborn-v0_8-paper\")\n",
        "    plt.rcParams[\"font.size\"] = 24\n",
        "    sns.set_context(\"paper\", font_scale=1.5)\n",
        "    sns.set_style(\n",
        "        \"ticks\",\n",
        "        {\n",
        "            \"axes.grid\": True,\n",
        "            \"grid.linestyle\": \"--\",\n",
        "            \"grid.alpha\": 0.6,\n",
        "            \"axes.spines.right\": False,\n",
        "            \"axes.spines.top\": False,\n",
        "            \"font.family\": \"serif\",\n",
        "            \"axes.labelpad\": 10,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    colors = [\n",
        "        \"#0173B2\",\n",
        "        \"#DE8F05\",\n",
        "        \"#029E73\",\n",
        "        \"#D55E00\",\n",
        "        \"#CC78BC\",\n",
        "        \"#CA9161\",\n",
        "        \"#FBAFE4\",\n",
        "    ]\n",
        "    sns.set_palette(colors)\n",
        "\n",
        "set_plot_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgA8nYn0X7bd"
      },
      "source": [
        "## 1. Data Loading and Exploration\n",
        "\n",
        "Let's start by loading the dataset and understanding its structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otGWcQn7X7bd"
      },
      "outputs": [],
      "source": [
        "!wget \"https://raw.githubusercontent.com/PickyBinders/ml-ai-summer-school-vib/refs/heads/main/0_dl_pytorch_intro/data/dimers_features.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZ5ZfHrlX7bd"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('dimers_features.csv')\n",
        "\n",
        "print(\"=== Dataset Overview ===\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Number of features: {len(df.columns) - 1}\")  # Excluding target\n",
        "\n",
        "# Check target distribution\n",
        "target_counts = df['physiological'].value_counts()\n",
        "print(f\"\\nTarget distribution:\")\n",
        "print(target_counts)\n",
        "print(f\"Physiological ratio: {target_counts[True] / len(df):.3f}\")\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "print(f\"\\nMissing values:\")\n",
        "print(missing_values[missing_values > 0])\n",
        "\n",
        "# Basic statistics of numerical features\n",
        "print(f\"\\nDataset info:\")\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "And2g_tgX7be"
      },
      "source": [
        "## 2. Feature Selection and Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOz716NsX7be"
      },
      "outputs": [],
      "source": [
        "categorical_cols = ['pdb-id', 'ID', 'SymmetryOp1', 'SymmetryOp2', 'gene', 'superfamily', 'pfam', \"difficult\", \"Unnamed: 0\"]\n",
        "target_col = 'physiological'\n",
        "\n",
        "# Get numerical columns\n",
        "numerical_cols = [col for col in df.columns if col not in categorical_cols + [target_col]]\n",
        "print(f\"Number of numerical features: {len(numerical_cols)}\")\n",
        "\n",
        "# Prepare features and target\n",
        "X = df[numerical_cols].values\n",
        "y = df[target_col].values\n",
        "\n",
        "print(f\"Features shape: X={X.shape}, y={y.shape}\")\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Train set: {X_train.shape}, Test set: {X_test.shape}\")\n",
        "print(f\"Train physiological ratio: {np.mean(y_train):.3f}\")\n",
        "print(f\"Test physiological ratio: {np.mean(y_test):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPkRGHViX7be"
      },
      "source": [
        "## 3. Normalization Techniques\n",
        "\n",
        "As we noticed, it might be important to normalize our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kThd7GGOX7be"
      },
      "outputs": [],
      "source": [
        "# Compare different normalization techniques\n",
        "print(\"=== Normalization Techniques Comparison ===\")\n",
        "\n",
        "# Test different normalizers\n",
        "normalizers = {\n",
        "    'StandardScaler': StandardScaler(),\n",
        "    'RobustScaler': RobustScaler(),\n",
        "    'MinMaxScaler': MinMaxScaler()\n",
        "}\n",
        "\n",
        "normalizer_results = {}\n",
        "\n",
        "for name, normalizer in normalizers.items():\n",
        "    print(f\"\\nTesting {name}...\")\n",
        "\n",
        "    # Fit and transform\n",
        "    X_train_norm = normalizer.fit_transform(X_train)\n",
        "    X_test_norm = normalizer.transform(X_test)\n",
        "\n",
        "    # Store results\n",
        "    normalizer_results[name] = {\n",
        "        'train': X_train_norm,\n",
        "        'test': X_test_norm,\n",
        "        'normalizer': normalizer\n",
        "    }\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"Train - Mean: {X_train_norm.mean():.3f}, Std: {X_train_norm.std():.3f}\")\n",
        "    print(f\"Test  - Mean: {X_test_norm.mean():.3f}, Std: {X_test_norm.std():.3f}\")\n",
        "\n",
        "# Use StandardScaler for further analysis (most common choice)\n",
        "best_normalizer = 'StandardScaler'\n",
        "X_train_norm = normalizer_results[best_normalizer]['train']\n",
        "X_test_norm = normalizer_results[best_normalizer]['test']\n",
        "\n",
        "print(f\"\\nUsing {best_normalizer} for further analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx1jNGXVX7bf"
      },
      "source": [
        "## 4. Advanced Neural Network Techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnLh2k-iX7bf"
      },
      "outputs": [],
      "source": [
        "# Base model class\n",
        "class AdvancedDimerClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, num_classes=2,\n",
        "                 dropout_rate=0.3, normalization='batch', activation='relu'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.normalization = normalization\n",
        "        self.activation = activation\n",
        "\n",
        "        layers = []\n",
        "        prev_size = input_size\n",
        "\n",
        "        # Build hidden layers with advanced techniques\n",
        "        for i, hidden_size in enumerate(hidden_sizes):\n",
        "            # Linear layer\n",
        "            layers.append(nn.Linear(prev_size, hidden_size))\n",
        "\n",
        "            # Normalization layer\n",
        "            if normalization == 'batch':\n",
        "                layers.append(nn.BatchNorm1d(hidden_size))\n",
        "            elif normalization == 'layer':\n",
        "                layers.append(nn.LayerNorm(hidden_size))\n",
        "\n",
        "\n",
        "            # Activation function\n",
        "            if activation == 'relu':\n",
        "                layers.append(nn.ReLU())\n",
        "            elif activation == 'leaky_relu':\n",
        "                layers.append(nn.LeakyReLU(0.1))\n",
        "            elif activation == 'sigmoid':\n",
        "                layers.append(nn.Sigmoid())\n",
        "\n",
        "            if i < len(hidden_sizes) - 1:  # No dropout after last hidden layer\n",
        "                if dropout_rate > 0:\n",
        "                    layers.append(nn.Dropout(dropout_rate))\n",
        "\n",
        "            prev_size = hidden_size\n",
        "\n",
        "        # Output layer\n",
        "        layers.append(nn.Linear(prev_size, num_classes))\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvIRDfH6X7bf"
      },
      "source": [
        "Create different models for our experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcsV8AmdX7bf"
      },
      "outputs": [],
      "source": [
        "def create_model_architectures(input_size):\n",
        "    \"\"\"Create different model architectures for comparison\"\"\"\n",
        "    architectures = {\n",
        "        'Logistic_Regression': [1],\n",
        "        'Large': [128, 64, 32, 16],\n",
        "    }\n",
        "    dropout_rates = [0.0, 0.25]\n",
        "    normalizations = [\"none\", \"batch\"]\n",
        "    activations = [\"relu\", \"sigmoid\"]\n",
        "\n",
        "    models = {}\n",
        "    for name, hidden_sizes in architectures.items():\n",
        "        for dropout_rate in dropout_rates:\n",
        "            for normalization in normalizations:\n",
        "                for activation in activations:\n",
        "                    model_name = f\"{name}_dropout_{dropout_rate}_norm_{normalization}_act_{activation}\"\n",
        "                    models[model_name] = AdvancedDimerClassifier(\n",
        "                        input_size=input_size,\n",
        "                        hidden_sizes=hidden_sizes,\n",
        "                        dropout_rate=dropout_rate,\n",
        "                        normalization=normalization,\n",
        "                        activation=activation\n",
        "                    )\n",
        "\n",
        "    return models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZWhvE4-X7bf"
      },
      "outputs": [],
      "source": [
        "input_size = X_train_norm.shape[1]\n",
        "models = create_model_architectures(input_size)\n",
        "\n",
        "print(\"=== Model Architectures ===\")\n",
        "for name, model in models.items():\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"{name} Model: {total_params:,} parameters ({trainable_params:,} trainable)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2KaCq9sX7bg"
      },
      "source": [
        "## 5. Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CrtVeo3X7bg"
      },
      "outputs": [],
      "source": [
        "def train_model_advanced(model, train_loader, test_loader,\n",
        "                        criterion, optimizer, scheduler,\n",
        "                        num_epochs, device, patience=15):\n",
        "    \"\"\"Advanced training with validation and early stopping\"\"\"\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for batch_features, batch_labels in train_loader:\n",
        "            batch_features = batch_features.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_features)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += batch_labels.size(0)\n",
        "            train_correct += (predicted == batch_labels).sum().item()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_features, batch_labels in test_loader:\n",
        "                batch_features = batch_features.to(device)\n",
        "                batch_labels = batch_labels.to(device)\n",
        "\n",
        "                outputs = model(batch_features)\n",
        "                loss = criterion(outputs, batch_labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += batch_labels.size(0)\n",
        "                val_correct += (predicted == batch_labels).sum().item()\n",
        "\n",
        "        # Calculate metrics\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(test_loader)\n",
        "        train_acc = train_correct / train_total\n",
        "        val_acc = val_correct / val_total\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
        "            scheduler.step(avg_val_loss)\n",
        "        else:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Store metrics\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        train_accuracies.append(train_acc)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        # Early stopping\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), f'best_model_{model.__class__.__name__}.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch:3d}: Train Loss = {avg_train_loss:.4f}, Train Acc = {train_acc:.4f}')\n",
        "            print(f'           Val Loss = {avg_val_loss:.4f}, Val Acc = {val_acc:.4f}')\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch}')\n",
        "            break\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0GtLX74X7bg"
      },
      "source": [
        "## 6. Data Preparation and Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQ8506JxX7bg"
      },
      "outputs": [],
      "source": [
        "class ProteinDimerDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = torch.FloatTensor(features)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "train_dataset = ProteinDimerDataset(X_train_norm, y_train)\n",
        "test_dataset = ProteinDimerDataset(X_test_norm, y_test)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Train loader: {len(train_loader)} batches\")\n",
        "print(f\"Test loader: {len(test_loader)} batches\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSfjz3WxX7bh"
      },
      "source": [
        "## 7. Model Training and Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3zPJswmX7bh"
      },
      "outputs": [],
      "source": [
        "num_epochs = 250\n",
        "learning_rate = 0.001\n",
        "weight_decay = 1e-4\n",
        "\n",
        "model_results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training {model_name} Model\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # We didn't use AdamW in the previous notebook, but it's a good optimizer for this task\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=10,\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    train_losses, test_losses, train_accuracies, test_accuracies = train_model_advanced(\n",
        "        model, train_loader, test_loader, criterion, optimizer, scheduler,\n",
        "        num_epochs, device, patience=20\n",
        "    )\n",
        "\n",
        "    # Store results\n",
        "    model_results[model_name] = {\n",
        "        'model': model,\n",
        "        'train_losses': train_losses,\n",
        "        'test_losses': test_losses,\n",
        "        'train_accuracies': train_accuracies,\n",
        "        'test_accuracies': test_accuracies\n",
        "    }\n",
        "\n",
        "    print(f\"{model_name} Test Accuracy: {test_accuracies[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppOglnh5X7bh"
      },
      "source": [
        "# 8. Performance Analysis and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzdfOLtXX7bh"
      },
      "outputs": [],
      "source": [
        "# Plot training progress for all models\n",
        "plt.figure(figsize=(30, 12))\n",
        "\n",
        "# Training losses\n",
        "plt.subplot(1, 3, 1)\n",
        "for name, results in model_results.items():\n",
        "    plt.plot(results['train_losses'], label=f'{name} Train', alpha=0.7)\n",
        "plt.title('Training Losses')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Validation losses\n",
        "plt.subplot(1, 3, 2)\n",
        "for name, results in model_results.items():\n",
        "    plt.plot(results['test_losses'], label=f'{name} Test', alpha=0.7)\n",
        "plt.title('Test Losses')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Training accuracies\n",
        "plt.subplot(1, 3, 3)\n",
        "for name, results in model_results.items():\n",
        "    plt.plot(results['train_accuracies'], label=f'{name} Train', alpha=0.7)\n",
        "plt.title('Training Accuracies')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYw_-Z3aX7bj"
      },
      "outputs": [],
      "source": [
        "# Plot training progress for all models\n",
        "plt.figure(figsize=(17, 35))\n",
        "\n",
        "# Validation accuracies\n",
        "plt.subplot(3, 1, 1)\n",
        "for name, results in model_results.items():\n",
        "    plt.plot(results['test_accuracies'], label=f'{name} Test', alpha=0.7)\n",
        "plt.title('Test Accuracies')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Model size vs performance\n",
        "plt.subplot(3, 1, 2)\n",
        "model_sizes = []\n",
        "test_accuracies = []\n",
        "for name, results in model_results.items():\n",
        "    model = results['model']\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    model_sizes.append(total_params)\n",
        "    test_accuracies.append(results['test_accuracies'][-1])\n",
        "\n",
        "plt.scatter(model_sizes, test_accuracies, s=100, alpha=0.7)\n",
        "for i, name in enumerate(model_results.keys()):\n",
        "    plt.annotate(name, (model_sizes[i], test_accuracies[i]),\n",
        "                xytext=(5, 5), textcoords='offset points')\n",
        "plt.xlabel('Number of Parameters')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.title('Model Size vs Performance')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Final test accuracies comparison\n",
        "plt.subplot(3, 1, 3)\n",
        "names = list(model_results.keys())\n",
        "accuracies = [model_results[name]['test_accuracies'][-1] for name in names]\n",
        "bars = plt.bar(names, accuracies, alpha=0.7)\n",
        "plt.title('Final Test Accuracies')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1)\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "             f'{acc:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjnmyMxcX7bj"
      },
      "source": [
        "## 9. Best Model Analysis\n",
        "\n",
        "Here we will try to better analyze performance of the best model, covering different classification metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tlle0UTX7bj"
      },
      "outputs": [],
      "source": [
        "best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['test_accuracies'][-1])\n",
        "best_model = model_results[best_model_name]['model']\n",
        "\n",
        "\n",
        "best_model.eval()\n",
        "test_probs = []\n",
        "test_labels = []\n",
        "test_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_features, batch_labels in test_loader:\n",
        "        batch_features = batch_features.to(device)\n",
        "        outputs = best_model(batch_features)\n",
        "        probs = F.softmax(outputs, dim=1)\n",
        "        test_probs.extend(probs[:, 1].cpu().numpy())  # Probability of positive class\n",
        "        test_labels.extend(batch_labels.numpy())\n",
        "        test_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "results_df = pd.DataFrame({\n",
        "    'Physiological_Probability': test_probs,\n",
        "    'Predicted_Class': test_preds,\n",
        "    'True_Class': test_labels,\n",
        "})\n",
        "\n",
        "print(\"Prediction Analysis:\")\n",
        "print(f\"Total predictions: {len(results_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCAlzfjSX7bj"
      },
      "outputs": [],
      "source": [
        "results_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqpQJTkBX7bj"
      },
      "source": [
        "### Basic metrics that require hard labels as predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7rBsMC_X7bk"
      },
      "outputs": [],
      "source": [
        "print(f\"Evaluating {best_model_name} model...\")\n",
        "\n",
        "# Calculate basic metrics\n",
        "accuracy = accuracy_score(test_labels, test_preds)\n",
        "precision = precision_score(test_labels, test_preds)\n",
        "recall = recall_score(test_labels, test_preds)\n",
        "f1 = f1_score(test_labels, test_preds)\n",
        "\n",
        "print(f\"\\nBasic Classification Metrics:\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgF1UQQBX7bk"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n=== Metric Interpretations ===\")\n",
        "print(f\"Accuracy:  Out of all predictions, what fraction were correct?\")\n",
        "print(f\"          {accuracy:.1%} of all predictions were correct\")\n",
        "print(f\"Precision: Out of all predicted positive cases, what fraction were actually positive?\")\n",
        "print(f\"          {precision:.1%} of predicted physiological dimers were actually physiological\")\n",
        "print(f\"Recall:    Out of all actual positive cases, what fraction did we catch?\")\n",
        "print(f\"          {recall:.1%} of actual physiological dimers were correctly identified\")\n",
        "print(f\"F1-Score:  Harmonic mean of precision and recall (balanced measure)\")\n",
        "print(f\"          {f1:.1%} - balanced performance between precision and recall\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYfOGPB3X7bk"
      },
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIIGokwAX7bk"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(test_labels, test_preds)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Basic confusion matrix\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Non-physiological', 'Physiological'],\n",
        "            yticklabels=['Non-physiological', 'Physiological'])\n",
        "plt.title(f'Confusion Matrix - {best_model_name}')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVvyQjjzX7bk"
      },
      "outputs": [],
      "source": [
        "cm_normalized = confusion_matrix(test_labels, test_preds, normalize='true')\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot 1: Basic confusion matrix\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues',\n",
        "            xticklabels=['Non-physiological', 'Physiological'],\n",
        "            yticklabels=['Non-physiological', 'Physiological'])\n",
        "plt.title(f'Confusion Matrix - {best_model_name}')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMOCErfiX7bk"
      },
      "source": [
        "### Distribution of predicted probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qus8IU_-X7bk"
      },
      "outputs": [],
      "source": [
        "sns.violinplot(x=test_labels, y=test_probs)\n",
        "sns.swarmplot(x=test_labels, y=test_probs, color='black', alpha=0.5)\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Probability of Physiological Class\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL_ans7rX7bk"
      },
      "source": [
        "### ROC curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJmNw6uZX7bl"
      },
      "outputs": [],
      "source": [
        "# Calculate ROC curve and AUC\n",
        "fpr, tpr, roc_thresholds = roc_curve(test_labels, test_probs)\n",
        "roc_auc = roc_auc_score(test_labels, test_probs)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
        "plt.ylabel('True Positive Rate (Sensitivity/Recall)')\n",
        "plt.title(f'ROC Curve - {best_model_name}')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Explain ROC curve\n",
        "print(f\"\\n=== ROC Curve Explanation ===\")\n",
        "print(f\"ROC (Receiver Operating Characteristic) Curve:\")\n",
        "print(f\"- X-axis: False Positive Rate (FPR) = FP/(FP+TN)\")\n",
        "print(f\"- Y-axis: True Positive Rate (TPR) = TP/(TP+FN) = Recall\")\n",
        "print(f\"- AUC (Area Under Curve) = {roc_auc:.3f}\")\n",
        "print(f\"- Perfect classifier: AUC = 1.0\")\n",
        "print(f\"- Random classifier: AUC = 0.5\")\n",
        "print(f\"- Our model: AUC = {roc_auc:.3f} ({'Good' if roc_auc > 0.8 else 'Fair' if roc_auc > 0.7 else 'Poor'} performance)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoaveqAiX7bl"
      },
      "outputs": [],
      "source": [
        "# Show different threshold points\n",
        "threshold_points = [0.2, 0.5, 0.8]\n",
        "print(f\"\\nThreshold Analysis:\")\n",
        "for threshold in threshold_points:\n",
        "    idx = np.argmin(np.abs(roc_thresholds - threshold))\n",
        "    fpr_val = fpr[idx]\n",
        "    tpr_val = tpr[idx]\n",
        "    print(f\"Threshold {threshold:.1f}: FPR={fpr_val:.3f}, TPR={tpr_val:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knXrldlUX7bl"
      },
      "source": [
        "### PR curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEBbjjKRX7bl"
      },
      "outputs": [],
      "source": [
        "precision_curve, recall_curve, pr_thresholds = precision_recall_curve(test_labels, test_probs)\n",
        "pr_auc = average_precision_score(test_labels, test_probs)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(recall_curve, precision_curve, color='red', lw=2, label=f'PR curve (AP = {pr_auc:.3f})')\n",
        "plt.axhline(y=np.mean(test_labels), color='navy', linestyle='--',\n",
        "            label=f'Random classifier (AP = {np.mean(test_labels):.3f})')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall (Sensitivity)')\n",
        "plt.ylabel('Precision')\n",
        "plt.title(f'Precision-Recall Curve - {best_model_name}')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Explain Precision-Recall curve\n",
        "print(f\"\\n=== Precision-Recall Curve Explanation ===\")\n",
        "print(f\"Precision-Recall Curve:\")\n",
        "print(f\"- X-axis: Recall (Sensitivity) = TP/(TP+FN)\")\n",
        "print(f\"- Y-axis: Precision = TP/(TP+FP)\")\n",
        "print(f\"- AP (Average Precision) = {pr_auc:.3f}\")\n",
        "print(f\"- Perfect classifier: AP = 1.0\")\n",
        "print(f\"- Random classifier: AP = proportion of positive class = {np.mean(test_labels):.3f}\")\n",
        "print(f\"- Our model: AP = {pr_auc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRpGLd8BX7bl"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nWhy PR Curve Matters for Imbalanced Data:\")\n",
        "print(f\"- Our dataset has {np.mean(test_labels):.1%} positive cases\")\n",
        "print(f\"- ROC curve can be misleading for imbalanced data\")\n",
        "print(f\"- PR curve focuses on the positive class performance\")\n",
        "print(f\"- Better metric for imbalanced classification problems\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJQLFDFNX7bl"
      },
      "source": [
        "## 10. Threshold Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOfZ6zd5X7bm"
      },
      "outputs": [],
      "source": [
        "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "threshold_metrics = []\n",
        "\n",
        "print(f\"\\n=== Threshold Analysis ===\")\n",
        "print(f\"Analyzing how different probability thresholds affect metrics:\")\n",
        "\n",
        "for threshold in thresholds:\n",
        "    # Apply threshold\n",
        "    predictions_threshold = (np.array(test_probs) >= threshold).astype(int)\n",
        "\n",
        "    # Calculate metrics\n",
        "    acc = accuracy_score(test_labels, predictions_threshold)\n",
        "    prec = precision_score(test_labels, predictions_threshold)\n",
        "    rec = recall_score(test_labels, predictions_threshold)\n",
        "    f1 = f1_score(test_labels, predictions_threshold)\n",
        "\n",
        "    threshold_metrics.append({\n",
        "        'threshold': threshold,\n",
        "        'accuracy': acc,\n",
        "        'precision': prec,\n",
        "        'recall': rec,\n",
        "        'f1': f1\n",
        "    })\n",
        "\n",
        "    print(f\"Threshold {threshold:.1f}: Acc={acc:.3f}, Prec={prec:.3f}, Rec={rec:.3f}, F1={f1:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_N5Wvq1X7bm"
      },
      "outputs": [],
      "source": [
        "threshold_df = pd.DataFrame(threshold_metrics)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(threshold_df['threshold'], threshold_df['accuracy'], 'o-', label='Accuracy')\n",
        "plt.xlabel('Classification Threshold')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs Threshold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(threshold_df['threshold'], threshold_df['precision'], 'o-', label='Precision')\n",
        "plt.xlabel('Classification Threshold')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision vs Threshold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(threshold_df['threshold'], threshold_df['recall'], 'o-', label='Recall')\n",
        "plt.xlabel('Classification Threshold')\n",
        "plt.ylabel('Recall')\n",
        "plt.xlabel('Classification Threshold')\n",
        "plt.ylabel('Recall')\n",
        "plt.title('Recall vs Threshold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.plot(threshold_df['threshold'], threshold_df['f1'], 'o-', label='F1-Score')\n",
        "plt.xlabel('Classification Threshold')\n",
        "plt.ylabel('F1-Score')\n",
        "plt.title('F1-Score vs Threshold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4Q4Q6nVX7bm"
      },
      "outputs": [],
      "source": [
        "optimal_idx = np.argmax(threshold_df['f1'])\n",
        "optimal_threshold = threshold_df.iloc[optimal_idx]['threshold']\n",
        "optimal_f1 = threshold_df.iloc[optimal_idx]['f1']\n",
        "\n",
        "print(f\"\\nOptimal Threshold Analysis:\")\n",
        "print(f\"Best F1-Score: {optimal_f1:.3f} at threshold {optimal_threshold:.1f}\")\n",
        "print(f\"This threshold balances precision and recall optimally\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX4yylQ3X7bm"
      },
      "source": [
        "## What are the most problematic observations?\n",
        "\n",
        "We have a column **difficult**, which indicates dimers that are harder to classify, we want to check how our model works for them:\n",
        "\n",
        "- Make another train/test split but where **difficult** observations are only in test set (the same 0.2 ratio).\n",
        "- Train the best model.\n",
        "- Test it and check how the model performed to **difficult** targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fo9eVE_sX7bm"
      },
      "outputs": [],
      "source": [
        "df[[\"physiological\", \"difficult\"]].value_counts()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python VIB course",
      "language": "python",
      "name": "protein_dl"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}